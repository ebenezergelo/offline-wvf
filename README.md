### Learning World Value Functions without Exploration

In our exploration of offline Reinforcement Learning (RL), our primary focus is on the development and understanding of a goal-oriented knowledge representation framework known as the World Value Function (WVF). To gauge the effectiveness of this framework, we conducted a comprehensive benchmarking study utilizing selected offline RL algorithms. Specifically, we examined the performance of offline Deep Q-Network (DQN) and Batch-Constrained deep Q-learning (BCQ), with a particular emphasis on their ability to learn goal-oriented value functions. Throughout our experiments, which encompassed both a 2D video game and a robotic environment, we delved into the realms of discrete and continuous action domains. Notably, the selected algorithms underwent modifications tailored to the learning of WVF. The success rates of learned WVFs were evaluated across varying replay data sizes, providing valuable insights into the efficiency of these algorithms under diverse conditions and domains. Our findings underscore the significance of possessing a large and diverse dataset for the effective learning of WVFs in a batch setting.
